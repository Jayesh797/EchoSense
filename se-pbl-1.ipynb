{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7767391,"sourceType":"datasetVersion","datasetId":4543574}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input/speech-dataset/BESD/ENGLISH'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-11T16:18:39.760863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import the libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\nimport librosa\nimport librosa.display\nfrom IPython.display import Audio\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path='/kaggle/input/speech-dataset/BESD/ENGLISH'\n# audio=[]\n# labels=[]\n# for classnames in os.listdir(path):\n#     classdir=os.path.join(path,classnames)\n#     for filename in os.listdir(classdir):\n#         audio.append(os.path.join(classdir,filename))\n#         a=filename.split('_')[1]\n#         b=a.split(' ')[1]\n#         labels.append(b)\n# print(labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"path='/kaggle/input/speech-dataset/BESD/ENGLISH'\naudio=[]\nlabels=[]\nfor classnames in os.listdir(path):\n    classdir=os.path.join(path,classnames)\n    for filename in os.listdir(classdir):\n        audio.append(os.path.join(classdir,filename))\n        labels.append(classnames)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame()\ndf['speech']=audio\ndf['labels']=labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['labels'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EXPLORATORY DATA ANALYSIS","metadata":{}},{"cell_type":"code","source":"def waveplot(data,sr,emotion):\n    plt.figure(figsize=(10,4))\n    plt.title(emotion,size=20)\n    librosa.display.waveplot(data,sr=sr)\n    plt.show()\n    \ndef spectrogram(data,sr,emotion):\n    x=librosa.stft(data)\n    xdb=librosa.amplitude_to_db(abs(x))\n    plt.figure(figsize=(10,4))\n    plt.title(emotion,size=20)\n    librosa.display.specshow(xdb,sr=sr,x_axis='time',y_axis='hz')\n    plt.colorbar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['speech'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y: This variable represents the audio time series, \n# which is a one-dimensional array containing the amplitude of the \n# audio signal sampled at regular intervals over time. \n# Each element of the array represents the amplitude of the audio signal at a specific time point.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"# Mel-Frequency Cepstral Coefficients\ndef extract_mfcc(filename):\n    y,sr=librosa.load(filename,duration=3,offset=0.5)\n    mfcc=np.mean(librosa.feature.mfcc(y=y,sr=sr,n_mfcc=40).T,axis=0)\n    return mfcc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y: This variable represents the audio time series, \n# which is a one-dimensional array containing the amplitude of the \n# audio signal sampled at regular intervals over time. \n# Each element of the array represents the amplitude of the audio signal at a specific time point.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_mfcc(df['speech'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_mfcc=df['speech'].apply(lambda x: extract_mfcc(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_mfcc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=[X for X in X_mfcc]\nX=np.array(X)\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=np.expand_dims(X,-1)\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nenc=OneHotEncoder()\ny=enc.fit_transform(df[['labels']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=y.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create LSTM model","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Dropout\n\nmodel=Sequential([\n    LSTM(123,return_sequences=False,input_shape=(40,1)),\n    Dense(64,activation='relu'),\n    Dropout(0.2),\n    Dense(32,activation='relu'),\n    Dropout(0.2),\n    Dense(6,activation='softmax')\n])\n\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(X,y,validation_split=0.2,epochs=100,batch_size=512,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PLOT THE GRAPHS","metadata":{}},{"cell_type":"code","source":"epochs=list(range(100))\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\n\nplt.plot(acc,epochs,label='train accuracy')\nplt.plot(val_acc,epochs,label='validation accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}